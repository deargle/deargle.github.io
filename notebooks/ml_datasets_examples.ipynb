{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching infosec-related machine learning datasets with scikit-learn\n",
    "=======================================================\n",
    "\n",
    "If the goal is to post a sample jupyter notebook online as a portfolio demonstration piece, then one of the first \"problems\" one encounters is, what dataset to use. If the goal of the portfolio piece is moreso to demonstrate model-building skills, and less-so domain knowledge of the dataset or associated feature-engineering skills, then starting with raw data would be tedious and distracting to a lean demonstration. One could theoretically prepare the data in a separate analysis, and then host a prepared dataset on e.g. github and fetch it using a url in the portfolio piece, but, again, this is distracting if data munging and feature engineering is not the primary goal for the demo.\n",
    "\n",
    "Fortunately, this is a common problem, and therefore, there exist standardized ways of fetching data.\n",
    "\n",
    "This notebook illustrates some of them. Specifically, security-data-related ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn.datasets\n",
    "-----------------------------\n",
    "\n",
    "Scikit-learn has [many methods](https://scikit-learn.org/stable/datasets/index.html) to load prepared or genereate random data with specified distributions.\n",
    "\n",
    "Mind that the \"normal\" traffic has a dadgum gotcha `.` at the end of its target value: `normal.`\n",
    "\n",
    "Also mind that, by default, the function only fetches 10% of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fetch_kddcup99`\n",
    "\n",
    "documentation for the dataset [here](https://kdd.ics.uci.edu/databases/kddcup99/task.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience function for describing a subset\n",
    "def describe_kddcup99_Ys(Y):\n",
    "    s = Y == b'normal.'\n",
    "    t = np.logical_not(s)\n",
    "\n",
    "    normal = Y[s]\n",
    "    abnormal = Y[t]\n",
    "\n",
    "    num_normal = normal.shape[0]\n",
    "    num_abnormal = abnormal.shape[0]\n",
    "    percent_abnormal = num_abnormal / ( num_normal + num_abnormal )\n",
    "\n",
    "    unique_abnormal = np.unique(abnormal)\n",
    "\n",
    "    print(\"{} normal traffic points\".format(Y[s].shape[0]))\n",
    "    print(\"{} abnormal traffic points\".format(Y[t].shape[0]))\n",
    "    print(\"percent abnormal {}\".format(percent_abnormal))\n",
    "    print(\"abnormal classes: {}\".format(unique_abnormal))\n",
    "    print('num_abnormal: {}'.format(unique_abnormal.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278 normal traffic points\n",
      "396743 abnormal traffic points\n",
      "percent abnormal 0.8030893423558918\n",
      "abnormal classes: [b'back.' b'buffer_overflow.' b'ftp_write.' b'guess_passwd.' b'imap.'\n",
      " b'ipsweep.' b'land.' b'loadmodule.' b'multihop.' b'neptune.' b'nmap.'\n",
      " b'perl.' b'phf.' b'pod.' b'portsweep.' b'rootkit.' b'satan.' b'smurf.'\n",
      " b'spy.' b'teardrop.' b'warezclient.' b'warezmaster.']\n",
      "num_abnormal: 22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "\n",
    "# no subset filtering, data of shape `(494021, 41)`, with \n",
    "# (97278,) normal traffic and \n",
    "# (396743,) abnormal traffic\n",
    "# making a ratio of 80% abnormal (that's a lot!!)\n",
    "\n",
    "x, Y = fetch_kddcup99(return_X_y=True)\n",
    "describe_kddcup99_Ys(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278 normal traffic points\n",
      "3377 abnormal traffic points\n",
      "percent abnormal 0.03355024588942427\n",
      "abnormal classes: [b'back.' b'ipsweep.' b'neptune.' b'nmap.' b'pod.' b'portsweep.' b'satan.'\n",
      " b'smurf.' b'teardrop.' b'warezclient.']\n",
      "num_abnormal: 10\n"
     ]
    }
   ],
   "source": [
    "# filter to all \"normal\" data plus only 3377 \"anomalous\" datapoints, \n",
    "# resulting in a feature dataset of shape `(100655, 41)` (3% non-normal traffic)\n",
    "\n",
    "x, Y = fetch_kddcup99(subset='SA', return_X_y=True)\n",
    "describe_kddcup99_Ys(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fetch_openml` -- Phishing Websites\n",
    "\n",
    "This openml dataset comes from [UCI](https://archive.ics.uci.edu/ml/datasets/phishing+websites), connected with three academic papers cited on the UCI page. Features are described [in this word doc](https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Phishing%20Websites%20Features.docx) hosted on uci.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas df colnames: Index(['having_IP_Address', 'URL_Length', 'Shortining_Service',\n",
      "       'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix',\n",
      "       'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length',\n",
      "       'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor',\n",
      "       'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL',\n",
      "       'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe',\n",
      "       'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank',\n",
      "       'Google_Index', 'Links_pointing_to_page', 'Statistical_report'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "x, Y = fetch_openml(data_id='4534', return_X_y=True) # dataset hosted , data_id taken from the url\n",
    "x.shape\n",
    "\n",
    "# use `as_frame` argument to get a pandas dataframe\n",
    "data, target = fetch_openml(data_id='4534', return_X_y=True, as_frame=True)\n",
    "print(\"pandas df colnames: {}\".format(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other OpenML security datasets\n",
    "* SPAM email database, [https://www.openml.org/d/44](https://www.openml.org/d/44)\n",
    "  - has text pre-featurized\n",
    "* Credit Card Fraud\n",
    "  - several on OpenML, but same dataset:\n",
    "      - [https://www.openml.org/d/42175](https://www.openml.org/d/42175)\n",
    "      - [https://www.openml.org/d/1597](https://www.openml.org/d/1597)\n",
    "  - This is the same dataset as the famous one hosted on kaggle, [here](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "    and also the same as the one used by [this AWS Sagemaker tutorial](https://aws.amazon.com/solutions/fraud-detection-using-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datahub.io datapackages -- Credit Card Fraud\n",
    "\n",
    "This is the same data as listed in the above \"Other OpenML security datasets\" page. Datahub imported this credit card fraud\n",
    "dataset from OpenML.\n",
    "\n",
    "They include code on their website for how to fetch and load their hosted data using various languages. Pandas example\n",
    "for the credit card fraud [here](https://www.datahub.io/machine-learning/creditcard#pandas)\n",
    "\n",
    "My gripe with this library is that it doesn't handle caching, whereas scikit-learn's `fetch_openml` [does](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml). I was going to say that one benefit of using the `datapackage` library is that it can load a dataset straight into a pandas dataframe, with attendant column names and types, but `fetch_openml` can do that too via the `as_frame` argument, since openml ARFF format affords specifying attribute names and types. \n",
    "\n",
    "So, pfft datahub.io. `fetch_openml` ftw."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
